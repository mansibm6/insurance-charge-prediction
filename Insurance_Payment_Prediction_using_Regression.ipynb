{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Insurance Payment Prediction using Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXvlZVO73bT5"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Read in the insurance dataset\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Vq3p5dic36Nt",
        "outputId": "c09a7300-5ccd-4f80-fbe9-ffc85eaf829e"
      },
      "source": [
        "# Check out the data\n",
        "insurance.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "      <th>charges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>female</td>\n",
              "      <td>27.900</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>southwest</td>\n",
              "      <td>16884.92400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>male</td>\n",
              "      <td>33.770</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1725.55230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>male</td>\n",
              "      <td>33.000</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>4449.46200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>male</td>\n",
              "      <td>22.705</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>21984.47061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>male</td>\n",
              "      <td>28.880</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>3866.85520</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age     sex     bmi  children smoker     region      charges\n",
              "0   19  female  27.900         0    yes  southwest  16884.92400\n",
              "1   18    male  33.770         1     no  southeast   1725.55230\n",
              "2   28    male  33.000         3     no  southeast   4449.46200\n",
              "3   33    male  22.705         0     no  northwest  21984.47061\n",
              "4   32    male  28.880         0     no  northwest   3866.85520"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAKI-RwS36_L"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECKJV5bl4Ce-"
      },
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "\n",
        "# Create column transformer (this will help us normalize/preprocess our data)\n",
        "ct = make_column_transformer(\n",
        "    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # get all values between 0 and 1\n",
        "    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"])\n",
        ")\n",
        "\n",
        "# Create X & y\n",
        "X = insurance.drop(\"charges\", axis=1)\n",
        "y = insurance[\"charges\"]\n",
        "\n",
        "# Build our train and test sets (use random state to ensure same split as before)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit column transformer on the training data only (doing so on test data would result in data leakage)\n",
        "ct.fit(X_train)\n",
        "\n",
        "# Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)\n",
        "X_train_normal = ct.transform(X_train)\n",
        "X_test_normal = ct.transform(X_test)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgX_lCwu4F4I",
        "outputId": "e6e20d49-3d61-4b27-db1b-929004d6738a"
      },
      "source": [
        "# Non-normalized and non-one-hot encoded data example\n",
        "X_train.loc[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                19\n",
              "sex            female\n",
              "bmi              27.9\n",
              "children            0\n",
              "smoker            yes\n",
              "region      southwest\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4xRsNMu4INP",
        "outputId": "d9c5aaca-ef2b-4468-bb94-0bc50d905655"
      },
      "source": [
        "# Normalized and one-hot encoded example\n",
        "X_train_normal[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.60869565, 0.10734463, 0.4       , 1.        , 0.        ,\n",
              "       1.        , 0.        , 0.        , 1.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvgvqciW4L9d",
        "outputId": "b8210e48-cde5-44b8-844f-5acbd9fc4296"
      },
      "source": [
        "# Notice the normalized/one-hot encoded shape is larger because of the extra columns\n",
        "X_train_normal.shape, X_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1070, 11), (1070, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPBX7ano4OAv",
        "outputId": "23626ff4-f296-497f-ec6b-0600be526dfb"
      },
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Build the model (3 layers, 100, 10, 1 units)\n",
        "insurance_model_3 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(100),\n",
        "  tf.keras.layers.Dense(10),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "insurance_model_3.compile(loss=tf.keras.losses.mae,\n",
        "                          optimizer=tf.keras.optimizers.Adam(),\n",
        "                          metrics=['mae'])\n",
        "\n",
        "# Fit the model for 200 epochs (same as insurance_model_2)\n",
        "insurance_model_3.fit(X_train_normal, y_train, epochs=200, verbose=1) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 13296.4671 - mae: 13296.4671\n",
            "Epoch 2/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 12948.4245 - mae: 12948.4245\n",
            "Epoch 3/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 12705.2201 - mae: 12705.2201\n",
            "Epoch 4/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 13369.7395 - mae: 13369.7395\n",
            "Epoch 5/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 13230.8567 - mae: 13230.8567\n",
            "Epoch 6/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 12995.1999 - mae: 12995.1999\n",
            "Epoch 7/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 12876.1059 - mae: 12876.1059\n",
            "Epoch 8/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 13004.0395 - mae: 13004.0395\n",
            "Epoch 9/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 12508.0465 - mae: 12508.0465\n",
            "Epoch 10/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 12304.9941 - mae: 12304.9941\n",
            "Epoch 11/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 12190.6080 - mae: 12190.6080\n",
            "Epoch 12/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 10948.1238 - mae: 10948.1238\n",
            "Epoch 13/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 11033.2710 - mae: 11033.2710\n",
            "Epoch 14/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 10209.3786 - mae: 10209.3786\n",
            "Epoch 15/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 9943.5374 - mae: 9943.5374\n",
            "Epoch 16/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 9393.3773 - mae: 9393.3773\n",
            "Epoch 17/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 8554.0274 - mae: 8554.0274\n",
            "Epoch 18/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 8463.0888 - mae: 8463.0888\n",
            "Epoch 19/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 8238.9919 - mae: 8238.9919\n",
            "Epoch 20/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7992.1423 - mae: 7992.1423\n",
            "Epoch 21/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7246.4891 - mae: 7246.4891\n",
            "Epoch 22/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7520.6745 - mae: 7520.6745\n",
            "Epoch 23/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7862.2013 - mae: 7862.2013\n",
            "Epoch 24/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7617.0075 - mae: 7617.0075\n",
            "Epoch 25/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 8118.1869 - mae: 8118.1869\n",
            "Epoch 26/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7381.5776 - mae: 7381.5776\n",
            "Epoch 27/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7759.2990 - mae: 7759.2990\n",
            "Epoch 28/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7961.3148 - mae: 7961.3148\n",
            "Epoch 29/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7357.1594 - mae: 7357.1594\n",
            "Epoch 30/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 7670.5824 - mae: 7670.5824\n",
            "Epoch 31/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7833.2158 - mae: 7833.2158\n",
            "Epoch 32/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7408.4353 - mae: 7408.4353\n",
            "Epoch 33/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7603.6621 - mae: 7603.6621\n",
            "Epoch 34/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7385.2734 - mae: 7385.2734\n",
            "Epoch 35/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7340.4551 - mae: 7340.4551\n",
            "Epoch 36/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7276.7009 - mae: 7276.7009\n",
            "Epoch 37/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7342.1807 - mae: 7342.1807\n",
            "Epoch 38/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 6918.2718 - mae: 6918.2718\n",
            "Epoch 39/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7252.1240 - mae: 7252.1240\n",
            "Epoch 40/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 7029.9498 - mae: 7029.9498\n",
            "Epoch 41/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6984.8112 - mae: 6984.8112\n",
            "Epoch 42/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6884.6928 - mae: 6884.6928\n",
            "Epoch 43/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6755.5828 - mae: 6755.5828\n",
            "Epoch 44/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6608.4541 - mae: 6608.4541\n",
            "Epoch 45/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6537.6250 - mae: 6537.6250\n",
            "Epoch 46/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6454.2749 - mae: 6454.2749\n",
            "Epoch 47/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6729.0266 - mae: 6729.0266\n",
            "Epoch 48/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6517.0466 - mae: 6517.0466\n",
            "Epoch 49/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6209.9380 - mae: 6209.9380\n",
            "Epoch 50/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6433.2527 - mae: 6433.2527\n",
            "Epoch 51/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6253.4749 - mae: 6253.4749\n",
            "Epoch 52/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6122.7599 - mae: 6122.7599\n",
            "Epoch 53/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 5797.8437 - mae: 5797.8437\n",
            "Epoch 54/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 6036.6884 - mae: 6036.6884\n",
            "Epoch 55/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 5688.6436 - mae: 5688.6436\n",
            "Epoch 56/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 5664.2595 - mae: 5664.2595\n",
            "Epoch 57/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 5857.4061 - mae: 5857.4061\n",
            "Epoch 58/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 5509.3852 - mae: 5509.3852\n",
            "Epoch 59/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 5316.6016 - mae: 5316.6016\n",
            "Epoch 60/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 5322.8677 - mae: 5322.8677\n",
            "Epoch 61/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 5458.1975 - mae: 5458.1975\n",
            "Epoch 62/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 5230.6231 - mae: 5230.6231\n",
            "Epoch 63/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 5084.5996 - mae: 5084.5996\n",
            "Epoch 64/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 4677.8217 - mae: 4677.8217\n",
            "Epoch 65/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 4707.1110 - mae: 4707.1110\n",
            "Epoch 66/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 4361.7458 - mae: 4361.7458\n",
            "Epoch 67/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 4333.7879 - mae: 4333.7879\n",
            "Epoch 68/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 4294.6888 - mae: 4294.6888\n",
            "Epoch 69/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 4365.7155 - mae: 4365.7155\n",
            "Epoch 70/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3931.4924 - mae: 3931.4924\n",
            "Epoch 71/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 4040.8881 - mae: 4040.8881\n",
            "Epoch 72/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3725.7781 - mae: 3725.7781\n",
            "Epoch 73/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 4041.0183 - mae: 4041.0183\n",
            "Epoch 74/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3849.4077 - mae: 3849.4077\n",
            "Epoch 75/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 4063.8180 - mae: 4063.8180\n",
            "Epoch 76/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3594.6083 - mae: 3594.6083\n",
            "Epoch 77/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3877.1156 - mae: 3877.1156\n",
            "Epoch 78/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3571.6357 - mae: 3571.6357\n",
            "Epoch 79/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3272.2002 - mae: 3272.2002\n",
            "Epoch 80/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3333.3793 - mae: 3333.3793\n",
            "Epoch 81/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3560.1417 - mae: 3560.1417\n",
            "Epoch 82/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 4082.1635 - mae: 4082.1635\n",
            "Epoch 83/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3690.6050 - mae: 3690.6050\n",
            "Epoch 84/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3491.1416 - mae: 3491.1416\n",
            "Epoch 85/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3196.7644 - mae: 3196.7644\n",
            "Epoch 86/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3696.4115 - mae: 3696.4115\n",
            "Epoch 87/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3584.8877 - mae: 3584.8877\n",
            "Epoch 88/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3717.8066 - mae: 3717.8066\n",
            "Epoch 89/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3437.4367 - mae: 3437.4367\n",
            "Epoch 90/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3799.4690 - mae: 3799.4690\n",
            "Epoch 91/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3660.1360 - mae: 3660.1360\n",
            "Epoch 92/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3743.4218 - mae: 3743.4218\n",
            "Epoch 93/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3536.3569 - mae: 3536.3569\n",
            "Epoch 94/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3439.5847 - mae: 3439.5847\n",
            "Epoch 95/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3570.7316 - mae: 3570.7316\n",
            "Epoch 96/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3734.5113 - mae: 3734.5113\n",
            "Epoch 97/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3444.5726 - mae: 3444.5726\n",
            "Epoch 98/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3465.0855 - mae: 3465.0855\n",
            "Epoch 99/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3225.0306 - mae: 3225.0306\n",
            "Epoch 100/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3421.0633 - mae: 3421.0633\n",
            "Epoch 101/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3517.1330 - mae: 3517.1330\n",
            "Epoch 102/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3469.4836 - mae: 3469.4836\n",
            "Epoch 103/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3250.2655 - mae: 3250.2655\n",
            "Epoch 104/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3700.7695 - mae: 3700.7695\n",
            "Epoch 105/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3445.0967 - mae: 3445.0967\n",
            "Epoch 106/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3625.0304 - mae: 3625.0304\n",
            "Epoch 107/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3781.3223 - mae: 3781.3223\n",
            "Epoch 108/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3602.7861 - mae: 3602.7861\n",
            "Epoch 109/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3813.4994 - mae: 3813.4994\n",
            "Epoch 110/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3938.7635 - mae: 3938.7635\n",
            "Epoch 111/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3809.7050 - mae: 3809.7050\n",
            "Epoch 112/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3549.6470 - mae: 3549.6470\n",
            "Epoch 113/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3577.8367 - mae: 3577.8367\n",
            "Epoch 114/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3748.3028 - mae: 3748.3028\n",
            "Epoch 115/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3593.5401 - mae: 3593.5401\n",
            "Epoch 116/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3820.6377 - mae: 3820.6377\n",
            "Epoch 117/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3539.3738 - mae: 3539.3738\n",
            "Epoch 118/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3555.8816 - mae: 3555.8816\n",
            "Epoch 119/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3674.5828 - mae: 3674.5828\n",
            "Epoch 120/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3811.3651 - mae: 3811.3651\n",
            "Epoch 121/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3819.3294 - mae: 3819.3294\n",
            "Epoch 122/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3608.0817 - mae: 3608.0817\n",
            "Epoch 123/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3486.8072 - mae: 3486.8072\n",
            "Epoch 124/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3441.4100 - mae: 3441.4100\n",
            "Epoch 125/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3706.9875 - mae: 3706.9875\n",
            "Epoch 126/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3593.7843 - mae: 3593.7843\n",
            "Epoch 127/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3648.6152 - mae: 3648.6152\n",
            "Epoch 128/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3572.3610 - mae: 3572.3610\n",
            "Epoch 129/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3729.5496 - mae: 3729.5496\n",
            "Epoch 130/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3588.1818 - mae: 3588.1818\n",
            "Epoch 131/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3475.3162 - mae: 3475.3162\n",
            "Epoch 132/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3823.5051 - mae: 3823.5051\n",
            "Epoch 133/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3732.0922 - mae: 3732.0922\n",
            "Epoch 134/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3716.7578 - mae: 3716.7578\n",
            "Epoch 135/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3573.0931 - mae: 3573.0931\n",
            "Epoch 136/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3544.5514 - mae: 3544.5514\n",
            "Epoch 137/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3437.4868 - mae: 3437.4868\n",
            "Epoch 138/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3784.2825 - mae: 3784.2825\n",
            "Epoch 139/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3460.7203 - mae: 3460.7203\n",
            "Epoch 140/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3598.6389 - mae: 3598.6389\n",
            "Epoch 141/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3864.1043 - mae: 3864.1043\n",
            "Epoch 142/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3498.1294 - mae: 3498.1294\n",
            "Epoch 143/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3564.5137 - mae: 3564.5137\n",
            "Epoch 144/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3703.2495 - mae: 3703.2495\n",
            "Epoch 145/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3854.3524 - mae: 3854.3524\n",
            "Epoch 146/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3622.4136 - mae: 3622.4136\n",
            "Epoch 147/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3641.0513 - mae: 3641.0513\n",
            "Epoch 148/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3532.8311 - mae: 3532.8311\n",
            "Epoch 149/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3385.0244 - mae: 3385.0244\n",
            "Epoch 150/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3696.7608 - mae: 3696.7608\n",
            "Epoch 151/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3627.8620 - mae: 3627.8620\n",
            "Epoch 152/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3510.4276 - mae: 3510.4276\n",
            "Epoch 153/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3455.5456 - mae: 3455.5456\n",
            "Epoch 154/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3713.6941 - mae: 3713.6941\n",
            "Epoch 155/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3461.3574 - mae: 3461.3574\n",
            "Epoch 156/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3421.8293 - mae: 3421.8293\n",
            "Epoch 157/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3363.8608 - mae: 3363.8608\n",
            "Epoch 158/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3645.1772 - mae: 3645.1772\n",
            "Epoch 159/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3499.4921 - mae: 3499.4921\n",
            "Epoch 160/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3570.0786 - mae: 3570.0786\n",
            "Epoch 161/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3474.6564 - mae: 3474.6564\n",
            "Epoch 162/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3583.9840 - mae: 3583.9840\n",
            "Epoch 163/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3390.4246 - mae: 3390.4246\n",
            "Epoch 164/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3523.7625 - mae: 3523.7625\n",
            "Epoch 165/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3437.2412 - mae: 3437.2412\n",
            "Epoch 166/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3674.9150 - mae: 3674.9150\n",
            "Epoch 167/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3747.5737 - mae: 3747.5737\n",
            "Epoch 168/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3649.0250 - mae: 3649.0250\n",
            "Epoch 169/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3444.5445 - mae: 3444.5445\n",
            "Epoch 170/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3792.1792 - mae: 3792.1792\n",
            "Epoch 171/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3566.9012 - mae: 3566.9012\n",
            "Epoch 172/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3626.4796 - mae: 3626.4796\n",
            "Epoch 173/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3667.2072 - mae: 3667.2072\n",
            "Epoch 174/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3446.7307 - mae: 3446.7307\n",
            "Epoch 175/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3701.3114 - mae: 3701.3114\n",
            "Epoch 176/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3674.0581 - mae: 3674.0581\n",
            "Epoch 177/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3545.1567 - mae: 3545.1567\n",
            "Epoch 178/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3716.4467 - mae: 3716.4467\n",
            "Epoch 179/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3275.6703 - mae: 3275.6703\n",
            "Epoch 180/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3729.8278 - mae: 3729.8278\n",
            "Epoch 181/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3744.1501 - mae: 3744.1501\n",
            "Epoch 182/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3588.6823 - mae: 3588.6823\n",
            "Epoch 183/200\n",
            "34/34 [==============================] - 0s 2ms/step - loss: 3335.0692 - mae: 3335.0692\n",
            "Epoch 184/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3391.9103 - mae: 3391.9103\n",
            "Epoch 185/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3315.4241 - mae: 3315.4241\n",
            "Epoch 186/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3637.4945 - mae: 3637.4945\n",
            "Epoch 187/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3639.0855 - mae: 3639.0855\n",
            "Epoch 188/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3529.6314 - mae: 3529.6314\n",
            "Epoch 189/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3285.8571 - mae: 3285.8571\n",
            "Epoch 190/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3579.1661 - mae: 3579.1661\n",
            "Epoch 191/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3425.1023 - mae: 3425.1023\n",
            "Epoch 192/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3375.7011 - mae: 3375.7011\n",
            "Epoch 193/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3411.6138 - mae: 3411.6138\n",
            "Epoch 194/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3680.5513 - mae: 3680.5513\n",
            "Epoch 195/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3362.7700 - mae: 3362.7700\n",
            "Epoch 196/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3333.7771 - mae: 3333.7771\n",
            "Epoch 197/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3654.9051 - mae: 3654.9051\n",
            "Epoch 198/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3230.2733 - mae: 3230.2733\n",
            "Epoch 199/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3548.0952 - mae: 3548.0952\n",
            "Epoch 200/200\n",
            "34/34 [==============================] - 0s 1ms/step - loss: 3667.0548 - mae: 3667.0548\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3d7dec8290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESRZH7oi4Uk-",
        "outputId": "24ce7309-6d7f-4a24-cd1a-6748553a941d"
      },
      "source": [
        "# Evaulate 3rd model\n",
        "insurance_model_3_loss, insurance_model_3_mae = insurance_model_3.evaluate(X_test_normal, y_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 2ms/step - loss: 3171.7632 - mae: 3171.7632\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}